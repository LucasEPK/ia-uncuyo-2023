---
title: "TP7 notebook"
output: html_notebook
---

1.  Bajar el archivo arbolado-mendoza-dataset.csv, el mismo se encuentra en formato CSV (comma separated values).

<!-- -->

a.  Seleccionar de manera uniformemente aleatoria el 20% del conjunto de datos y crear un nuevo archivo con el nombre de arbolado-mendoza-dataset-validation.csv y el 80% restante con el nombre de arbolado-mendoza-dataset-train.csv

```{r}
library(readr)
library(tidyverse)
library(caret)

arboladoDS <- read_csv("../data/arbolado-mza-dataset.csv")
arboladoRows <- nrow(arboladoDS)
# Here we shuffle the dataset's rows randomly
shuffledDS <- arboladoDS[sample(1:arboladoRows),]

# Here we calculate how many rows is 20% of the total rows, and also the remaining 80%
N20Percent <- round(arboladoRows*0.20)
Nremaining <- arboladoRows-N20Percent
# Here we split the shuffled dataset in a half, validationDS takes the first 20% of rows while trainDS takes the remaining 80% of rows
validationDS <- slice_head(shuffledDS, n = N20Percent)
trainDS <- slice_tail(shuffledDS, n = Nremaining)

# Writes to memory the resulting train and validation datasets
write.csv(validationDS, "../data/arbolado-mza-dataset-validation.csv", row.names=FALSE)
write.csv(trainDS, "../data/arbolado-mza-dataset-train.csv", row.names=FALSE)
```

2.  A partir del archivo arbolado-mendoza-dataset-train.csv responder las siguientes preguntas:

<!-- -->

a.  Cual es la distribución de las clase inclinacion_peligrosa?
b.  ¿Se puede considerar alguna sección más peligrosa que otra?
c.  ¿Se puede considerar alguna especie más peligrosa que otra?

IMPORTANTE: para responder cada una de estas preguntas se deberá generar una visualización/gráfico que justifique la respuesta.

```{r}
library(readr)
library(tidyverse)
library(caret)
library(ggplot2)

arboladoTrainDS <- read_csv("../data/arbolado-mza-dataset-train.csv")

inclinacionPeligrosaColumn <- select(arboladoTrainDS, inclinacion_peligrosa)
distribucionPeligrosa <- table(inclinacionPeligrosaColumn)
distribucionPeligrosa
barplot(distribucionPeligrosa)


# Here we group by species and calculate the probability of picking randomly a dangerously inclined tree by specie
especiesPeligrosas <- arboladoTrainDS %>% group_by(especie) %>%
  summarise(
    porcentaje_inclinacion_peligrosa = sum(inclinacion_peligrosa)/length(inclinacion_peligrosa)
  )

ggplot(especiesPeligrosas, aes(x=especie, y=porcentaje_inclinacion_peligrosa)) + 
  geom_bar(stat = "identity")+
  coord_flip()

# Here we group by sections and calculate the probability of picking randomly a dangerously inclined tree by sections
seccionesPeligrosas <- arboladoTrainDS %>% group_by(nombre_seccion) %>%
  summarise(
    porcentaje_inclinacion_peligrosa = sum(inclinacion_peligrosa)/length(inclinacion_peligrosa)
  )

ggplot(seccionesPeligrosas, aes(x=nombre_seccion, y=porcentaje_inclinacion_peligrosa)) + 
  geom_bar(stat = "identity")+
  coord_flip()
```

3.  A partir del archivo arbolado-mendoza-dataset-train.csv:

<!-- -->

b.  Generar un histograma de frecuencia para la variable circ_tronco_cm. Probar con diferentes números de bins.\
c.  Repetir el punto b) pero separando por la clase de la variable inclinación_peligrosa?\
d.  Crear una nueva variable categórica de nombre circ_tronco_cm_cat a partir circ_tronco_cm, en donde puedan asignarse solo 4 posibles valores [ muy alto, alto, medio, bajo ]. Utilizar la información del punto b. para seleccionar los puntos de corte para cada categoría. Guardar el nuevo dataframe bajo el nombre de arbolado-mendoza-dataset-circ_tronco_cm-train.csv

Los criterios de corte son: Si es menor que 90, bajo si es mayor o igual que 90 y menor que 170, medio si es mayor o igual que 170 y menor que 240, alto si es mayor o igual que 240, muy alto

```{r}
library(readr)
library(tidyverse)
library(caret)

arboladoTrainDS <- read_csv("../data/arbolado-mza-dataset-train.csv")
# Makes different histograms with different bins values
hist(arboladoTrainDS$circ_tronco_cm, main = "Histograma de circ_tronco_cm", xlab = "Circunferencia tronco en cm", ylab = "Frecuencia", col = 4)
hist(arboladoTrainDS$circ_tronco_cm, breaks = 50, main = "Histograma de circ_tronco_cm con 50 bins", xlab = "Circunferencia tronco en cm", ylab = "Frecuencia", col = 4)
hist(arboladoTrainDS$circ_tronco_cm, breaks = 100, main = "Histograma de circ_tronco_cm con 100 bins", xlab = "Circunferencia tronco en cm", ylab = "Frecuencia", col = 4)
hist(arboladoTrainDS$circ_tronco_cm, breaks = 200, main = "Histograma de circ_tronco_cm con 200 bins", xlab = "Circunferencia tronco en cm", ylab = "Frecuencia", col = 4)

# Select circ_tronco_cm where inclinacion_peligrosa is equal to 0
circTroncoSinPeligro <- subset(arboladoTrainDS$circ_tronco_cm, arboladoTrainDS$inclinacion_peligrosa == 0)
# Select circ_tronco_cm where inclinacion_peligrosa is equal to 1
circTroncoPeligrosa <- subset(arboladoTrainDS$circ_tronco_cm, arboladoTrainDS$inclinacion_peligrosa == 1)

hist(circTroncoSinPeligro, main = "Histograma de circ_tronco_cm SIN inclinación peligrosa", xlab = "Circunferencia tronco en cm", ylab = "Frecuencia", breaks = 20, col = 3)
hist(circTroncoPeligrosa, main = "Histograma de circ_tronco_cm CON inclinación peligrosa", xlab = "Circunferencia tronco en cm", ylab = "Frecuencia", col = 2)

circTroncoDS <- arboladoTrainDS %>% 
  mutate(circ_tronco_cm_cat = ifelse(circ_tronco_cm < 90, "bajo",
                                ifelse(circ_tronco_cm < 170, "medio", 
                                       ifelse(circ_tronco_cm < 240, "alto", "muy alto")
                                       )
                                )
         )

write.csv(circTroncoDS, "../data/arbolado-mza-dataset-circ_tronco_cm-train.csv", row.names=FALSE)
```

4. Clasificador aleatório:

a. Implementar una función que dado un conjunto de observaciones (data.frame) genere una nueva columna de nombre prediction_prob con un valor aleatorio entre 0 y 1.  
b. Implementar una función de nombre random_classifier, que reciba como parámetro el dataframe generado con anterioridad y a partir de la columna predictions_prob genere una nueva columna prediction_class bajo el siguiente criterio:  
If predictions_prob > 0.5 then prediction_class=1 else prediction_class=0  
La función deberá devolver el dataframe original junto a la nueva columna generada.  
c. Cargar el archivo arbolado-mendoza-dataset-validation.csv como un data.frame y aplicarle la función random_classifier  
d. A partir de la columna recientemente generada y la columna con la clase (inclinación peligrosa) calcular utilizando lenguaje R (dplyr) el número de:  
i. Número de árboles CON inclinación peligrosa que fueron correctamente predicho como peligrosos por el modelo/algoritmo. (True Positive)  
ii. Número de árboles SIN inclinación peligrosa que fueron correctamente predicho como no peligrosos por el modelo. (True Negative)  
iii. Número de árboles SIN inclinación peligrosa que fueron incorrectamente predicho como peligrosos según el modelo. (False Positives)  
iv. Número de árboles CON inclinación peligrosa que fueron incorrectamente predicho como no peligrosos según el modelo. (False Negatives)

```{r}
library(readr)
library(tidyverse)
library(caret)

new_prediction_prob <- function(dataframe) { 
  # Function that creates a new column in the dataframe given with random values between 0 and 1
  predictionProbDS <- dataframe %>% 
    mutate(prediction_prob = runif(n()))
  return(predictionProbDS)
}

random_classifier <- function(ogDataframe, predictionProbDF) {
  # Function that creates a new column in the original dataframe given with random values between 0 and 1 decided by the prediction_prob in the predictionProbDF given
  # If the prediction_prob is more than 5 the new column will have a value of 1, else it will have a value of 0
  predictionClassCol <- predictionProbDF %>% 
    mutate(prediction_class = ifelse(prediction_prob > 0.5, 1, 0)) %>%
    select(prediction_class)
  
  predictionClassDF <- ogDataframe %>% 
    add_column(predictionClassCol)
  
  return(predictionClassDF)
}

make_confusion_matrix <- function(dataframe) {
  # Makes a confusion matrix using dplyr with the prediction_class of the dataframe and the inclinacion_peligrosa of the dataframe
  
  # Calculates True Positives
  truePositives <- dataframe %>%
    filter(prediction_class == 1 & prediction_class == inclinacion_peligrosa) %>%
    nrow()
  # Calculates True Negatives
  trueNegatives <- dataframe %>%
    filter(prediction_class == 0 & prediction_class == inclinacion_peligrosa) %>%
    nrow()
  # Calculates False Positives
  falsePositives <- dataframe %>%
    filter(prediction_class == 1 & prediction_class != inclinacion_peligrosa) %>%
    nrow()
  # Calculates False Negatives
  falseNegatives <- dataframe %>%
    filter(prediction_class == 0 & prediction_class != inclinacion_peligrosa) %>%
    nrow()
  matrix <- matrix(c(truePositives, falseNegatives, falsePositives, trueNegatives), nrow = 2, ncol = 2, byrow = TRUE)
  # specify the column names and row names of matrix
  colnames(matrix) = c('Predicted: Yes','Predicted: No')
  rownames(matrix) <- c('Actual: Yes','Actual: No')
  
  confusionMatrix1 <- as.table(matrix)
  
  return(confusionMatrix1)
}

arboladoValidationDS <- read_csv("../data/arbolado-mza-dataset-validation.csv")

# Creates a new column called prediction_prob with a random number between 0 and 1 in the arboladoValidationDS dataframe
predictionProbDS <- new_prediction_prob(arboladoValidationDS)
predictionProbDS

# Creates a new column called prediction_class with random predictions of 0 and 1
predictionClassDF <- random_classifier(arboladoValidationDS, predictionProbDS)
predictionClassDF

confMatrix <- make_confusion_matrix(predictionClassDF)
print("")
confMatrix

```
5. Clasificador por clase mayoritaria:  
a. Implementar una función de nombre biggerclass_classifier, que reciba como parámetro el dataframe generado con anterioridad y genere una nueva columna de nombre prediction_class en donde se asigne siempre de la clase mayoritaria  
La función deberá devolver el dataframe original junto a la nueva columna generada.  
b. Repetir los puntos 4.c y 4.d pero aplicando la nueva función biggerclass_classifier

```{r}
library(readr)
library(tidyverse)
library(caret)

biggerclass_classifier <- function(dataframe) {
  # Calculates which value is the most present in inclinacion_peligrosa and assigns that new value to a new column called prediction_class, then returns the dataframe with this new column
  inclinacion_peligrosa1 <- dataframe %>% filter(inclinacion_peligrosa == 1) %>% nrow()
  inclinacion_peligrosa0 <- dataframe %>% filter(inclinacion_peligrosa == 0) %>% nrow()
  
  result = ifelse(inclinacion_peligrosa1 > inclinacion_peligrosa0, 1, 0)
  
  newDataframe <- dataframe %>% mutate(prediction_class = result)
  return(newDataframe)
}

arboladoValidationDS <- read_csv("../data/arbolado-mza-dataset-validation.csv")

predictionDS <- biggerclass_classifier(arboladoValidationDS)
predictionDS

cMatrix <- make_confusion_matrix(predictionDS)
print("")
cMatrix
```

6. A partir de una matriz de confusión es posible calcular distintas métricas que nos permiten determinar la calidad del modelo de clasificación.  
Utilizar la siguiente imagen como guía crear funciones para calcular: Accuracy, Precision, Sensitivity, Specificity y calcularlas para las matrices de confusión generadas en los puntos 4 y 5.

```{r}
calculate_sensitivity <- function(cMatrix) {
  # Given a confusion matrix, calculates and returns the sensitivity
  tp <- cMatrix[1,1]
  fn <- cMatrix[1,2]
  sensitivity1 <- tp/(tp+fn)
  return(sensitivity1)
}

calculate_specificity <- function(cMatrix) {
  # Given a confusion matrix, calculates and returns the specificity  
  tn <- cMatrix[2,2]
  fp <- cMatrix[2,1]
  specificity1 <- tn/(tn+fp)
  return(specificity1)
}

calculate_precision <- function(cMatrix) {
  # Given a confusion matrix, calculates and returns the precision
  tp <- cMatrix[1,1]
  fp <- cMatrix[2,1]
  precision1 <- tp/(tp+fp)
  return(precision1)
}

calculate_negativePV <- function(cMatrix) {
  # Given a confusion matrix, calculates and returns the negative predictive value
  tn <- cMatrix[2,2]
  fn <- cMatrix[1,2]
  negativePV <- tn/(tn+fn)
  return(negativePV)
}

calculate_accuracy <- function(cMatrix) {
  # Given a confusion matrix, calculates and returns the accuracy
  tp <- cMatrix[1,1]
  fn <- cMatrix[1,2]
  tn <- cMatrix[2,2]
  fp <- cMatrix[2,1]
  
  accuracy1 <- (tp+tn)/(tp+tn+fp+fn)
  return(accuracy1)
}

calculate_metrics <- function(cMatrix) {
  # Calculates sensitivity, specificity, precision, negative predictive value and the accuracy of the given confusion matrix and returns it as an array
  sensitivity1 <- calculate_sensitivity(cMatrix)
  specificity1 <- calculate_specificity(cMatrix)
  precision1 <- calculate_precision(cMatrix)
  negativePV <- calculate_negativePV(cMatrix)
  accuracy1 <- calculate_accuracy(cMatrix)
  
  metrics <- array(data = c(sensitivity1, specificity1, precision1, negativePV, accuracy1), dim = c(5))
  return(metrics)
}

print_metrics <- function(metrics) {
  print("Sensitivity:")
  print(metrics[1])
  print("Specificity:")
  print(metrics[2])
  print("Precision:")
  print(metrics[3])
  print("Negative Predictive Value:")
  print(metrics[4])
  print("Accuracy:")
  print(metrics[5])
}

randomClassifierMetrics <- calculate_metrics(confMatrix)
print("Random classifier metrics:")
print_metrics(randomClassifierMetrics)
print("")
print("Bigger class classifier metrics:")
biggerclassClassifierMetrics <- calculate_metrics(cMatrix)
print_metrics(biggerclassClassifierMetrics)


```

7. Validación cruzada (Cross validation) (k-folds):  
La validación cruzada es una técnica para estimar el error de generalización de un algoritmo/modelo de machine learning. La técnica consiste en (previo realizar una mezcla aleatoria) separar el conjunto de datos en k partes (normalmente denominadas folds). Luego en la primera iteración se utilizan k-1 partes para entrenar E1 y se utiliza la restante para test. El proceso se repite por k iteraciones utilizando en cada una diferentes conjuntos de entrenamiento y test. (Ver figura)  
a. Crear una función de nombre create_folds() que reciba como parámetro un dataframe y la cantidad de folds y devuelva una lista de R con la siguiente estructura:  
list(Fold1=c(...), Fold2=c(..),... Fold10=c())  
Donde Fold1 va a contender los índices del dataframe que fueron seleccionados para el primer fold, y así con los demás.
b. Crear una función de nombre cross_validation() que reciba como parámetro un data frame y un número de folds y entrene un modelo de árbol de decisión (utilizar paquete rpart) para cada uno de los posibles conjuntos de entrenamiento y calcule las métricas: Accuracy, Precision, Sensitivity, Specificity para cada uno de los posibles conjuntos de tests. Devolver media y desviación estándar.

```{r}
library(readr)
library(tidyverse)
library(rpart)

calculate_list_mean <- function(list, nFolds) {
  # Calculates the mean of a list of metrics, for every metric and returns the mean of the metrics in an array
  
  sensitivitySum <- 0
  specificitySum <- 0
  precisionSum <- 0
  negativePVsum <- 0
  accuracySum <- 0
    
  for (fold in 1:nFolds) { 
    
    metrics <- list[[fold]]
    
    sensitivitySum <- sensitivitySum + metrics[1]
    specificitySum <- specificitySum + metrics[2]
    precisionSum <- precisionSum + metrics[3]
    negativePVsum <- negativePVsum + metrics[4]
    accuracySum <- accuracySum + metrics[5]
    
  }
  
  sensitivityMean <- sensitivitySum / nFolds
  specificityMean <- specificitySum / nFolds
  precisionMean <- precisionSum / nFolds
  negativePVmean <- negativePVsum / nFolds
  accuracyMean <- accuracySum / nFolds
  
  resultMetrics <- array(data = c(sensitivityMean, specificityMean, precisionMean, negativePVmean, accuracyMean), dim = c(5))
  return(resultMetrics)
}

calculate_list_sd <-function(list, nFolds) {
  # Calculates the standard deviation of a list of metrics, for every metric and returns the standard deviation of the metrics in an array
  sensitivityC <- c()
  specificityC <- c()
  precisionC <- c()
  negativePVc <- c()
  accuracyC <- c()
    
  for (fold in 1:nFolds) { 
    
    metrics <- list[[fold]]
    
    sensitivityC <- c(sensitivitySum, metrics[1])
    specificityC <- c(specificitySum, metrics[2])
    precisionC <- c(precisionSum, metrics[3])
    negativePVc <- c(negativePVsum, metrics[4])
    accuracyC <- c(accuracySum, metrics[5])
    
  }
  
  sensitivitySD <- sd(sensitivityC)
  specificitySD <- sd(specificityC)
  precisionSD <- sd(precisionC)
  negativePVsd <- sd(negativePVc)
  accuracySD <- sd(accuracyC)
  
  resultMetrics <- array(data = c(sensitivitySD, specificitySD, precisionSD, negativePVsd, accuracySD), dim = c(5))
  return(resultMetrics)
}

create_folds <- function(dataframe, nFolds) {
  rows <- nrow(dataframe)
  
  elementsPerFold <- floor(rows/nFolds)
  
  foldList <- list()
  for (fold in 1:nFolds) {
    
      indexBegin <- (fold-1) * elementsPerFold + 1
    # This is divided so in the last fold all the remaining elements caused by the ceiling of the elementsPerFold are placed in the last fold
    if(fold != nFolds) {
      indexEnd <- (fold) * elementsPerFold
    } else {
      indexEnd <- rows
    }
      
      foldList[[paste("Fold", fold)]] <- c(indexBegin:indexEnd)
  }
  
  return(foldList)
}

cross_validation <- function(dataframe, nFolds) {
  foldIndexList <- create_folds(dataframe, nFolds)
  
  metricsList <- list()
  
  for (fold in 1:nFolds) {
    # Here we remove the validation fold
    ignoredIndexes <- foldIndexList[[fold]]
    
    trainFoldsDF <- dataframe[-ignoredIndexes,]
    validationFoldDF <- dataframe[ignoredIndexes,]
    # Here we calculate the prediction for the other folds
    train_formula <- formula(inclinacion_peligrosa~id+especie+ultima_modificacion+altura+circ_tronco_cm+diametro_tronco+long+lat+seccion+nombre_seccion+area_seccion)
    tree_model <- rpart(train_formula, data = trainFoldsDF)
    predProbs <- predict(tree_model, validationFoldDF, type="class")
    
    predColumn <- ifelse(predProbs$predictions >=0.5,1,0)
    validation_comparison <- data.frame(id=validationFoldDF$id, inclinacion_peligrosa = validationFoldDF$inclinacion_peligrosa, prediction_class = predColumn)

    cMatrix <- make_confusion_matrix(validation_comparison)
    metrics <- calculate_metrics(cMatrix)
    
    metricsList[[paste("Metric", fold)]] <- metrics
  }
  
  listMean <- calculate_list_mean(metricsList, nFolds)
  listSD <- calculate_list_sd(metricsList, nFolds)
  print_metrics(listMean)
  print_metrics(listSD)
}


# Load the train dataset
arboladoTrainDS <- read_csv("../data/arbolado-mza-dataset-train.csv")

# Shuffles the dataset
set.seed(211)
rows <- sample(nrow(arboladoTrainDS))
shuffledTrainDS <- arboladoTrainDS[rows,]

nFolds <- 10

cross_validation(shuffledTrainDS, nFolds)


# Test to see how lists and dataframes work
#folddd <- list(fold1 = c(1,2,3,4), fold2 = c(5,6,7,8))
#firstFoldIndexes <- folddd[[1]]
#folddd[[2]]

#my_data <- data.frame(
#  ID = 1:8,
#  Name = c("Alice", "Bob", "Charlie", "David", "Eva", "Frank", "Grace", "Henry"),
#  Age = c(25, 30, 22, 35, 28, 40, 26, 32),
#  Score = c(92, 85, 78, 94, 88, 75, 90, 82)
#)


#lol <- my_data[-firstFoldIndexes,]
#lol
#bruh1 <- array(data = c("a", "b", "c", "d", "e"), dim = c(5))
#bruh2 <- array(data = c("f", "g", "h", "i", "l"), dim = c(5))
#result <- list(data1 = bruh1, data2 = bruh2)
#result$data1[2]


#xd <- c()
#xd
#xd <- c(xd, 1)
#xd
```